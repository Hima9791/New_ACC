#############################################
# MODULE: RESULT COMBINER
# Purpose: Merges the output of the fixed processing pipeline
#          with the output of the detailed analysis pipeline.
#############################################

import pandas as pd
import os
import traceback
import streamlit as st # For error/warning/debug messages


# MODIFIED combine_results
def combine_results(processed_df: pd.DataFrame, # Changed input to DataFrame
                    analysis_file: str,
                    output_file="final_combined.xlsx"):
    """
    Combines processed data (from fixed pipeline DataFrame) with detailed analysis
    results (read from Excel file). Merges based on the original value string.

    Args:
        processed_df (pd.DataFrame): DataFrame output from the fixed pipeline.
                                     Expected to have 'Main Key' column.
        analysis_file (str): Path to the Excel file generated by the detailed analysis pipeline.
                             Expected to have a 'Value' column matching 'Main Key'.
        output_file (str): Path to save the final combined Excel file.

    Returns:
        str or None: The path to the output file on success, None on failure.
    """
    st.write("DEBUG: Starting combine_results function...")
    try:
        # --- Input Validation ---
        # 1) Check processed DataFrame
        if not isinstance(processed_df, pd.DataFrame):
            st.error("Combine Error: Processed data input must be a pandas DataFrame.")
            return None
        if processed_df.empty:
            st.warning("Combine Warning: Processed DataFrame is empty. Combining may result in an empty file.")
            # Allow proceeding, but the result might be empty or just analysis data if merge fails.
        if 'Main Key' not in processed_df.columns:
             st.error("Combine Error: 'Main Key' column missing in processed data DataFrame. Cannot merge.")
             return None

        # 2) Read analysis results file
        try:
            if not os.path.exists(analysis_file):
                 st.error(f"Combine Error: Analysis file not found at '{analysis_file}'.")
                 return None
            df_analysis = pd.read_excel(analysis_file)
            st.write(f"DEBUG: Read analysis file '{analysis_file}'. Shape: {df_analysis.shape}")
        except Exception as e:
            st.error(f"Combine Error: Failed to read analysis file '{analysis_file}': {e}")
            return None

        if df_analysis.empty:
            st.warning("Combine Warning: Analysis DataFrame read from file is empty.")
            # If analysis is empty, the merge will likely result in NaNs for analysis columns. Proceed.

        # Check for 'Value' column in analysis data (needed for merge key)
        if 'Value' not in df_analysis.columns:
             st.error("Combine Error: 'Value' column missing in analysis data. Cannot use it as merge key.")
             # Alternative merge strategy? On index? Too risky. Require 'Value'.
             return None

        st.write("DEBUG: Columns in processed_df:", processed_df.columns.tolist())
        st.write("DEBUG: Columns in df_analysis:", df_analysis.columns.tolist())

        # --- Define Columns to Merge ---
        # Select relevant columns from the analysis DataFrame to merge into the processed DataFrame.
        # This avoids merging redundant columns (like original data copied into analysis)
        # and allows control over which analysis fields are included.
        analysis_cols_to_merge = [
            "Value", # Key column from analysis side, MUST be first? No, needed for merge key.
            "Classification",
            "DetailedValueType",
            "Identifiers",
            "SubValueCount",
            "ConditionCount", "MainItemCount", # Added MainItemCount
            "HasRangeInMain", "HasMultiValueInMain", "HasRangeInCondition", "HasMultipleConditions",
            "Normalized Unit",
            "Absolute Unit",
            "MainUnits", "MainDistinctUnitCount", "MainUnitsConsistent",
            "ConditionUnits", "ConditionDistinctUnitCount", "ConditionUnitsConsistent",
            "OverallUnitConsistency",
            "ParsingErrorFlag",
            "SubValueUnitVariationSummary",
            "MainNumericValues", "ConditionNumericValues", "MainMultipliers", "ConditionMultipliers",
            "MainBaseUnits", "ConditionBaseUnits", "NormalizedMainValues", "NormalizedConditionValues",
            "MinNormalizedValue", "MaxNormalizedValue", "SingleUnitForAllSubs", "AllDistinctUnitsUsed"
        ]
        # Filter this list to only include columns that *actually exist* in df_analysis to avoid KeyErrors
        existing_analysis_cols = [col for col in analysis_cols_to_merge if col in df_analysis.columns]
        if not existing_analysis_cols or "Value" not in existing_analysis_cols:
             st.error("Combine Error: Analysis data is missing key columns ('Value' and/or others) needed for merging.")
             return None
        st.write(f"DEBUG: Merging analysis columns: {existing_analysis_cols}")

        # Select only the required columns from df_analysis for the merge
        df_analysis_subset = df_analysis[existing_analysis_cols].copy()
        # Drop duplicates in analysis subset based on 'Value' if necessary?
        # Assume detailed analysis output has one row per unique 'Value'. If not, duplicates might cause issues.
        df_analysis_subset.drop_duplicates(subset=['Value'], keep='first', inplace=True)


        # --- Perform the Merge ---
        # Merge processed_df (left) with the selected analysis columns (right)
        # Use 'Main Key' from processed_df and 'Value' from df_analysis_subset as keys.
        df_merged = processed_df.merge(
            df_analysis_subset,
            how="left", # Keep all rows from processed_df, add analysis data where match found
            left_on="Main Key",
            right_on="Value",
            suffixes=("_processed", "_analysis") # Add suffixes to distinguish potentially duplicated column names AFTER merge (e.g., if 'Classification' existed in both)
        )
        st.write(f"DEBUG: Merged DataFrame shape: {df_merged.shape}")


        # --- Clean up Merged Columns ---
        # 1. Drop the redundant 'Value' column that came from the analysis side ('Value_analysis' if suffix added, or 'Value' if no suffix needed)
        if 'Value_analysis' in df_merged.columns:
            df_merged.drop(columns=["Value_analysis"], inplace=True)
        elif 'Value' in df_merged.columns and 'Main Key' in df_merged.columns and 'Value' in existing_analysis_cols and '_analysis' not in df_merged.columns.tolist():
             # If suffixes weren't added (e.g., 'Value' only existed on right), 'Value' is the merge key from analysis.
             # We have 'Main Key' from left and 'Value' from right. Keep 'Main Key' as the primary identifier?
             # Let's check if they are identical where 'Value' is not null, and maybe drop 'Value'.
             # Simpler: Keep both for now, let final column selection handle it. 'Main Key' is the definitive one.
             pass

        # 2. Handle other potential duplicated columns (e.g., if 'Classification' was in both DFs)
        # The suffixes=("_processed", "_analysis") should handle this. We might have 'Classification_processed' and 'Classification_analysis'.
        # Decide which one to keep or how to resolve conflicts.
        # Typically, the analysis version ('_analysis') might be more accurate or detailed.
        # Let's prioritize the analysis version if duplicates exist.
        cols_to_drop = []
        cols_to_rename = {}
        for col in existing_analysis_cols: # Iterate through cols we brought from analysis
             processed_col = f"{col}_processed"
             analysis_col = f"{col}_analysis"
             if processed_col in df_merged.columns and analysis_col in df_merged.columns:
                  # Both exist, prioritize analysis version
                  cols_to_drop.append(processed_col)       # Drop the one from processed side
                  cols_to_rename[analysis_col] = col # Rename the analysis one back to original name
             elif analysis_col in df_merged.columns and col not in df_merged.columns:
                   # Only analysis version exists (with suffix), rename it
                   cols_to_rename[analysis_col] = col

        if cols_to_drop:
             df_merged.drop(columns=cols_to_drop, inplace=True)
             st.write(f"DEBUG: Dropped processed-side duplicate columns: {cols_to_drop}")
        if cols_to_rename:
             df_merged.rename(columns=cols_to_rename, inplace=True)
             st.write(f"DEBUG: Renamed analysis-side columns: {cols_to_rename}")

        st.write(f"DEBUG: Columns after merge & cleanup: {df_merged.columns.tolist()}")


        # --- Define and Reorder Final Columns ---
        # Start with columns from the processed_df structure (Main Key, Category, Attribute, Code, Value)
        # Then add the analysis columns in a logical order.
        # Then add original data columns (excluding 'Value' if 'Main Key' is kept).
        # Finally, add the 'Sheet' column.

        # Identify original columns from input (present in processed_df excluding fixed pipeline additions)
        fixed_pipeline_cols = {"Main Key", "Category", "Attribute", "Code", "Value", "Sheet"} # Base columns added/used by fixed pipeline
        original_input_cols = [col for col in processed_df.columns if col not in fixed_pipeline_cols and not col.endswith("_processed")]
        # Ensure we don't include 'Value' from original if 'Main Key' is the primary key. Let's keep 'Main Key'.
        if "Value" in original_input_cols:
             original_input_cols.remove("Value")


        final_columns_order = [
            # Key Identifier
            "Main Key",
            # Core Fixed Pipeline Output
            "Category",         # Category from fixed pipeline (might differ slightly from detailed Classification)
            "Attribute",        # Value or Unit or Info
            "Code",             # SV-V, RV-Un, M1-SV-V etc.
            "Value",            # Parsed value/unit string for this specific Code/Attribute row
            # Core Detailed Analysis Classification
            "Classification",   # Detailed classification string
            "DetailedValueType",# Summary string like "Single Value [M:1][C:1]"
            "Normalized Unit",  # String with numbers replaced by '$'
            "Absolute Unit",    # Resolved base units in structure (e.g., "V @ A")
            "Identifiers",      # Content from parentheses
            # Structural Analysis Details
             "SubValueCount", "ConditionCount", "MainItemCount",
             "HasRangeInMain", "HasMultiValueInMain", "HasRangeInCondition", "HasMultipleConditions",
            # Unit Analysis Details
             "MainUnits", "MainDistinctUnitCount", "MainUnitsConsistent",
             "ConditionUnits", "ConditionDistinctUnitCount", "ConditionUnitsConsistent",
             "OverallUnitConsistency", "SingleUnitForAllSubs", "AllDistinctUnitsUsed",
             "SubValueUnitVariationSummary",
             # Numeric Analysis Details
             "MainNumericValues", "ConditionNumericValues", "MainMultipliers", "ConditionMultipliers",
             "MainBaseUnits", "ConditionBaseUnits", "NormalizedMainValues", "NormalizedConditionValues",
             "MinNormalizedValue", "MaxNormalizedValue",
            # Processing Flags/Metadata
             "ParsingErrorFlag",
             "Sheet", # Sheet name from original file
            # Add original columns from the input file at the end
            *sorted(original_input_cols) # Spread the sorted list of original columns
        ]

        # Filter this list to only include columns that actually exist in df_merged
        existing_final_columns = [col for col in final_columns_order if col in df_merged.columns]

        # Add any columns from df_merged that were missed (shouldn't happen if list is comprehensive)
        missed_cols = [col for col in df_merged.columns if col not in existing_final_columns]
        final_ordered_cols = existing_final_columns + sorted(missed_cols)

        # Ensure no duplicates in final list (belt and suspenders)
        final_ordered_cols = list(dict.fromkeys(final_ordered_cols))


        st.write(f"DEBUG: Final column order being applied: {final_ordered_cols}")
        df_final = df_merged[final_ordered_cols]

        # --- Write Final Output ---
        df_final.to_excel(output_file, index=False, engine='openpyxl')
        print(f"[✓] Final combined file saved to '{output_file}'")
        st.write(f"DEBUG: Final combined file saved to '{output_file}'. Shape: {df_final.shape}")
        return output_file # Return path on success

    except Exception as e:
        st.error(f"An unexpected error occurred during combine_results: {e}")
        st.error(traceback.format_exc())
        return None
